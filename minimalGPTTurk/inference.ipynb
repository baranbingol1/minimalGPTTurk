{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02feae33-079a-4324-aa6c-98af81cef1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from gpt2 import GPT2Model, get_gpt2_cfg\n",
    "from samplers import Sampler, GreedySampler, TopKSampler, NucleusSampler\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "ckpt_path = './out/ckpt.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09d4670e-3453-4e93-ab26-0195f976690c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx: torch.Tensor, max_new_tokens: int, context_size: int, sampler: Sampler, eos_id=None) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Sampler classlarÄ±yla beraber bir sonraki tokenlarÄ± oluÅŸturma fonksiyonu.\n",
    "    \"\"\"\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        # sadece son token'a odaklanÄ±yoruz Ã§Ã¼nkÃ¼ ondan Ã¶ncekilerin metin Ã¼retirken Ã¶nemi yok.\n",
    "        # inference yaparken bÃ¶yle bir mini-optimizasyon yapÄ±labiliyor.\n",
    "        # indexlemeye anlayabilmek iÃ§in logitlerin [batch_size, sequence_length, vocab_size] ÅŸeklinde olduÄŸunu hatÄ±rlamakta fayda var.\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        idx_next = sampler.sample(logits)\n",
    "        \n",
    "        # batchteki herhangi bir token eos_id'ye eÅŸitse metin Ã¼retimini durdur.\n",
    "        if eos_id and (idx_next == eos_id).any():\n",
    "            break\n",
    "\n",
    "        # sample olan indexlerle beraber eski indexleri dÃ¶ndÃ¼rÃ¼yoruz\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1) ÅŸeklinde.\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2585858-da7b-4387-9e1e-59ad395a9e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = get_gpt2_cfg('very_tiny', vocab_size=141)\n",
    "model = GPT2Model(cfg).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49893d11-863f-4593-a377-e1066e12bae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# eÄŸitlen modelin parametrelerini yÃ¼klÃ¼yoruz.\n",
    "ckpt = torch.load(ckpt_path)\n",
    "model.load_state_dict(ckpt['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04f4067a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer'Ä±n kullandÄ±ÄŸÄ± vocab ile encode ve decode fonksiyonlarÄ±nÄ± oluÅŸturuyoruz.\n",
    "with open('vocab.json', encoding='utf-8') as json_file:\n",
    "    vocab = json.load(json_file)\n",
    "\n",
    "char_to_idx = vocab\n",
    "idx_to_char = {idx: ch for ch, idx in vocab.items()}\n",
    "\n",
    "def encode_text(text, char_to_idx):\n",
    "    return torch.tensor([char_to_idx.get(ch, char_to_idx['<bk>']) for ch in text], dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "def decode_text(tensor, idx_to_char):\n",
    "    return ''.join([idx_to_char[idx.item()] for idx in tensor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40e8cd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_ctx = \"Ey acemi dudaklÄ± yar\"\n",
    "encoded_tensor = encode_text(start_ctx, char_to_idx).to(device)\n",
    "max_new_tokens = 250\n",
    "context_size = cfg.block_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd6832a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "# ÅŸimdi ise eÄŸlenceli olan metin Ã¼retme kÄ±smÄ±ndayÄ±z.\n",
    "# unutmayinki hiperparametrelerle oynayÄ±p daha iyi sonuÃ§lar almak mÃ¼mkÃ¼n.\n",
    "greedy_sampler = GreedySampler()\n",
    "generated_greedy = generate(model, encoded_tensor, max_new_tokens, context_size, greedy_sampler)\n",
    "\n",
    "top_k_sampler = TopKSampler(temperature=0.8, k=10)\n",
    "generated_top_k = generate(model, encoded_tensor, max_new_tokens, context_size, top_k_sampler)\n",
    "\n",
    "nucleus_sampler = NucleusSampler(temperature=1.0, p=0.9)\n",
    "generated_nucleus = generate(model, encoded_tensor, max_new_tokens, context_size, nucleus_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0143ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampler ismi: Greedy\n",
      "------\n",
      "Ey acemi dudaklÄ± yarattÄ±\n",
      "Bir karanlÄ±k bir karanlÄ±k gibi\n",
      "Bir karanlÄ±k gibi bakmadan geliyor\n",
      "Bir gÃ¼n bakmadan bakmadan geliyor\n",
      "Bir gÃ¼n bir gÃ¼n bakmadan geliyor geliyor\n",
      "Bir gÃ¼n bir gÃ¼n bakmadan geliyor geliyor\n",
      "Bir gÃ¼n bir gÃ¼n bakmadan geliyor geliyor\n",
      "Bir gÃ¼n bir gÃ¼n bakmad\n",
      "\n",
      "Sampler ismi: Top-k\n",
      "------\n",
      "Ey acemi dudaklÄ± yardan bilmez mi yar? \n",
      "\n",
      "YalnÄ±zlÄ±ÄŸÄ±m bu gemileri de artÄ±k dÃ¼ÅŸ gÃ¼nlerim.\n",
      "GÃ¶Ã§meden bende aÄŸlattÄ±m.\n",
      "YÃ¼zÃ¼mÃ¼ de sen yÃ¼zÃ¼nde.\n",
      "ArtÄ±k sevda kal bÄ±kma kaldÄ±m; \n",
      "DalgalandÄ±m.\n",
      "GÃ¶lgesi arÄ±yor kalbime...\n",
      "\n",
      "Denet kalkan olmayÄ±, \n",
      "KÃ¼llere girmeyi benzemeyi; \n",
      "Akar elinden \n",
      "\n",
      "Sampler ismi: Nucleus(Top-p)\n",
      "------\n",
      "Ey acemi dudaklÄ± yardan arayÄ±ÅŸÄ± ey Nermin? \n",
      "Hak yÃ¢dÄ± bir halkas eÄŸniyor bunun bilmiyor...\n",
      "â€” Allah kulaklarÄ±na size ki geliyor, aldÄ±ÄŸÄ±nÄ±z! \n",
      "â€” Maâ€™bedin Ã§Ä±ktÄ±ÄŸÄ± hep diye mÃ¼slÃ¼man sÄ±rlar diye bakÄ±yor...\n",
      "â€” Ortak verdik...\n",
      "â€” Ne dedin? \n",
      "â€” SÄ±rtÄ±ndan sÄ±kÄ±ntÄ±nÄ±z keder? \n",
      "â€” Â«BekÂ»\n",
      "D\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sonuÃ§larÄ± ekrana bastÄ±ralÄ±m\n",
    "sampler_name = ['Greedy', 'Top-k', 'Nucleus(Top-p)']\n",
    "sampler_results = [generated_greedy, generated_top_k, generated_nucleus] \n",
    "\n",
    "for name, result in zip(sampler_name, sampler_results):\n",
    "    decoded_text = decode_text(result[0], idx_to_char) \n",
    "    print(f\"Sampler ismi: {name}\")\n",
    "    print(\"------\")\n",
    "    print(decoded_text)\n",
    "    print() # gÃ¶rsellik iÃ§in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bb8b6b",
   "metadata": {},
   "source": [
    "### sonuÃ§lar eh iÅŸte... diyelim ğŸ¯"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

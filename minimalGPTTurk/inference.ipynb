{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02feae33-079a-4324-aa6c-98af81cef1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from gpt2 import GPT2Model, get_gpt2_cfg\n",
    "from samplers import Sampler, GreedySampler, TopKSampler, NucleusSampler\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "ckpt_path = './out/ckpt.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09d4670e-3453-4e93-ab26-0195f976690c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx: torch.Tensor, max_new_tokens: int, context_size: int, sampler: Sampler, eos_id=None) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Sampler classlarıyla beraber bir sonraki tokenları oluşturma fonksiyonu.\n",
    "    \"\"\"\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        # sadece son token'a odaklanıyoruz çünkü ondan öncekilerin metin üretirken önemi yok.\n",
    "        # inference yaparken böyle bir mini-optimizasyon yapılabiliyor.\n",
    "        # indexlemeye anlayabilmek için logitlerin [batch_size, sequence_length, vocab_size] şeklinde olduğunu hatırlamakta fayda var.\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        idx_next = sampler.sample(logits)\n",
    "        \n",
    "        # batchteki herhangi bir token eos_id'ye eşitse metin üretimini durdur.\n",
    "        if eos_id and (idx_next == eos_id).any():\n",
    "            break\n",
    "\n",
    "        # sample olan indexlerle beraber eski indexleri döndürüyoruz\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1) şeklinde.\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2585858-da7b-4387-9e1e-59ad395a9e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = get_gpt2_cfg('very_tiny', vocab_size=141)\n",
    "model = GPT2Model(cfg).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49893d11-863f-4593-a377-e1066e12bae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# eğitlen modelin parametrelerini yüklüyoruz.\n",
    "ckpt = torch.load(ckpt_path)\n",
    "model.load_state_dict(ckpt['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04f4067a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer'ın kullandığı vocab ile encode ve decode fonksiyonlarını oluşturuyoruz.\n",
    "with open('vocab.json', encoding='utf-8') as json_file:\n",
    "    vocab = json.load(json_file)\n",
    "\n",
    "char_to_idx = vocab\n",
    "idx_to_char = {idx: ch for ch, idx in vocab.items()}\n",
    "\n",
    "def encode_text(text, char_to_idx):\n",
    "    return torch.tensor([char_to_idx.get(ch, char_to_idx['<bk>']) for ch in text], dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "def decode_text(tensor, idx_to_char):\n",
    "    return ''.join([idx_to_char[idx.item()] for idx in tensor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40e8cd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_ctx = \"Ey acemi dudaklı yar\"\n",
    "encoded_tensor = encode_text(start_ctx, char_to_idx).to(device)\n",
    "max_new_tokens = 250\n",
    "context_size = cfg.block_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd6832a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "# şimdi ise eğlenceli olan metin üretme kısmındayız.\n",
    "# unutmayinki hiperparametrelerle oynayıp daha iyi sonuçlar almak mümkün.\n",
    "greedy_sampler = GreedySampler()\n",
    "generated_greedy = generate(model, encoded_tensor, max_new_tokens, context_size, greedy_sampler)\n",
    "\n",
    "top_k_sampler = TopKSampler(temperature=0.8, k=10)\n",
    "generated_top_k = generate(model, encoded_tensor, max_new_tokens, context_size, top_k_sampler)\n",
    "\n",
    "nucleus_sampler = NucleusSampler(temperature=1.0, p=0.9)\n",
    "generated_nucleus = generate(model, encoded_tensor, max_new_tokens, context_size, nucleus_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0143ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampler ismi: Greedy\n",
      "------\n",
      "Ey acemi dudaklı yarattı\n",
      "Bir karanlık bir karanlık gibi\n",
      "Bir karanlık gibi bakmadan geliyor\n",
      "Bir gün bakmadan bakmadan geliyor\n",
      "Bir gün bir gün bakmadan geliyor geliyor\n",
      "Bir gün bir gün bakmadan geliyor geliyor\n",
      "Bir gün bir gün bakmadan geliyor geliyor\n",
      "Bir gün bir gün bakmad\n",
      "\n",
      "Sampler ismi: Top-k\n",
      "------\n",
      "Ey acemi dudaklı yardan bilmez mi yar? \n",
      "\n",
      "Yalnızlığım bu gemileri de artık düş günlerim.\n",
      "Göçmeden bende ağlattım.\n",
      "Yüzümü de sen yüzünde.\n",
      "Artık sevda kal bıkma kaldım; \n",
      "Dalgalandım.\n",
      "Gölgesi arıyor kalbime...\n",
      "\n",
      "Denet kalkan olmayı, \n",
      "Küllere girmeyi benzemeyi; \n",
      "Akar elinden \n",
      "\n",
      "Sampler ismi: Nucleus(Top-p)\n",
      "------\n",
      "Ey acemi dudaklı yardan arayışı ey Nermin? \n",
      "Hak yâdı bir halkas eğniyor bunun bilmiyor...\n",
      "— Allah kulaklarına size ki geliyor, aldığınız! \n",
      "— Ma’bedin çıktığı hep diye müslüman sırlar diye bakıyor...\n",
      "— Ortak verdik...\n",
      "— Ne dedin? \n",
      "— Sırtından sıkıntınız keder? \n",
      "— «Bek»\n",
      "D\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sonuçları ekrana bastıralım\n",
    "sampler_name = ['Greedy', 'Top-k', 'Nucleus(Top-p)']\n",
    "sampler_results = [generated_greedy, generated_top_k, generated_nucleus] \n",
    "\n",
    "for name, result in zip(sampler_name, sampler_results):\n",
    "    decoded_text = decode_text(result[0], idx_to_char) \n",
    "    print(f\"Sampler ismi: {name}\")\n",
    "    print(\"------\")\n",
    "    print(decoded_text)\n",
    "    print() # görsellik için."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bb8b6b",
   "metadata": {},
   "source": [
    "### sonuçlar eh işte... diyelim 🐯"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

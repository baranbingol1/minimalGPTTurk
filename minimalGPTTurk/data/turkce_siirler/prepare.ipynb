{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29a10156-4690-4377-bf0b-d6d85332e699",
   "metadata": {},
   "source": [
    "**Bu notebookta train.py için huggingface de bulunan bir türkçe şiirler veri setini indirip tokenize edip train.py için hazır hale getiriyoruz.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "947965fb-8242-4df0-9d9c-50ce93b9a153",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "from datasets import load_dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d5e4a99-23a0-423b-9aa5-7afeacaff5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.encoding_for_model(\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac5e6272-2b13-43f6-ac1e-ebfdaa782c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('beratcmn/turkish-poems-cleaned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d451705-d823-47f9-bee3-4c0356467693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['poem'],\n",
       "        num_rows: 3720\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['poem'],\n",
       "        num_rows: 1241\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unwanted_columns = ['title', 'rating', 'id', 'poet']\n",
    "split_dataset = dataset['train'].train_test_split(test_size=0.25, seed=1337)\n",
    "split_dataset = split_dataset.remove_columns(unwanted_columns)\n",
    "split_dataset['val'] = split_dataset.pop('test') # test splitinin ismini val olarak değiştiriyoruz\n",
    "split_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a94030b-71cd-4e17-a69e-a70286575dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(example):\n",
    "        ids = enc.encode_ordinary(example['poem'])\n",
    "        ids.append(enc.eot_token)\n",
    "        out = {'ids': ids, 'len': len(ids)}\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a966044-6b79-42a3-9982-e35e87ce986f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = split_dataset.map(\n",
    "        process,\n",
    "        remove_columns=['poem'],\n",
    "        desc=\"tokenizing\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ac33e3d-c95b-4720-8a0f-dfd9f5b11a21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9146,\n",
       " 77,\n",
       " 400,\n",
       " 5872,\n",
       " 2484,\n",
       " 90069,\n",
       " 445,\n",
       " 22215,\n",
       " 198,\n",
       " 9146,\n",
       " 2484,\n",
       " 90069,\n",
       " 445,\n",
       " 22215,\n",
       " 57453,\n",
       " 132929,\n",
       " 412,\n",
       " 13518,\n",
       " 2144,\n",
       " 67617,\n",
       " 89,\n",
       " 142256,\n",
       " 270,\n",
       " 3163,\n",
       " 84000,\n",
       " 198,\n",
       " 64220,\n",
       " 2730,\n",
       " 10847,\n",
       " 51931,\n",
       " 572,\n",
       " 1916,\n",
       " 817,\n",
       " 1916,\n",
       " 159385,\n",
       " 279,\n",
       " 44,\n",
       " 1220,\n",
       " 4356,\n",
       " 72,\n",
       " 293,\n",
       " 25173,\n",
       " 448,\n",
       " 13140,\n",
       " 10709,\n",
       " 198,\n",
       " 2223,\n",
       " 13140,\n",
       " 10709,\n",
       " 15587,\n",
       " 11,\n",
       " 13739,\n",
       " 13739,\n",
       " 1305,\n",
       " 190439,\n",
       " 61070,\n",
       " 198,\n",
       " 10162,\n",
       " 573,\n",
       " 1431,\n",
       " 3742,\n",
       " 859,\n",
       " 7666,\n",
       " 612,\n",
       " 809,\n",
       " 188523,\n",
       " 10709,\n",
       " 412,\n",
       " 33,\n",
       " 3966,\n",
       " 10709,\n",
       " 3314,\n",
       " 28182,\n",
       " 36855,\n",
       " 177027,\n",
       " 143325,\n",
       " 158563,\n",
       " 75,\n",
       " 1842,\n",
       " 279,\n",
       " 41750,\n",
       " 2717,\n",
       " 11744,\n",
       " 5781,\n",
       " 24216,\n",
       " 572,\n",
       " 412,\n",
       " 44,\n",
       " 407,\n",
       " 3403,\n",
       " 4091,\n",
       " 178281,\n",
       " 28690,\n",
       " 1678,\n",
       " 4692,\n",
       " 198,\n",
       " 4599,\n",
       " 149296,\n",
       " 3087,\n",
       " 6782,\n",
       " 68534,\n",
       " 27339,\n",
       " 28314,\n",
       " 2832,\n",
       " 198,\n",
       " 42,\n",
       " 22245,\n",
       " 61994,\n",
       " 514,\n",
       " 199999]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized['train'][0]['ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d765a81-4920-4bc1-b9b6-125b672ecd10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train.bin oluşturuluyor...: 100%|██████████| 128/128 [00:00<00:00, 857.77it/s]\n",
      "val.bin oluşturuluyor...: 100%|██████████| 128/128 [00:00<00:00, 972.77it/s] \n"
     ]
    }
   ],
   "source": [
    "for split, dset in tokenized.items():\n",
    "        arr_len = np.sum(dset['len'], dtype=np.uint64)\n",
    "        filename = f'{split}.bin'\n",
    "        dtype = np.uint32\n",
    "        arr = np.memmap(filename, dtype=dtype, mode='w+', shape=(arr_len,))\n",
    "        total_batches = 128\n",
    "\n",
    "        idx = 0\n",
    "        for batch_idx in tqdm(range(total_batches), desc=f'{filename} oluşturuluyor...'):\n",
    "            batch = dset.shard(num_shards=total_batches, index=batch_idx, contiguous=True).with_format('numpy')\n",
    "            arr_batch = np.concatenate(batch['ids'])\n",
    "            arr[idx : idx + len(arr_batch)] = arr_batch\n",
    "            idx += len(arr_batch)\n",
    "        arr.flush()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
